# Deep Learning Courses

## Even semester

### CSE Department

#### **CS 419 M** : <u>Introduction to Machine Learning</u>
**Syllabus**
Basic classification/regression techniques such as Naive Bayes', decision trees, SVMs, boosting/bagging and linear/logistic regression, maximum likelihood estimates, regularization, basics of statistical learning theory, perceptron rule/multi-layer perceptrons, backpropagation, brief introduction to deep learning models, dimensionality reduction techniques like PCA and LDA, unsupervised learning: k-means clustering, gaussian mixture models, selected topics from natural/spoken language processing, computer vision, etc.

**Pre-requisites**
None

####  **CS 621** : <u>Artificial Intelligence</u>
**Syllabus**
Knowledge Representation: The First Order Predicate Logic, Production Systems, Semantic Nets, Frames and Scripts Formalisms. Resolution in Predicate Logic, Unification, Strategies for Resolution by Refutation. Knowledge Acquisition and learning: Learning from examples and analogy, Rote learning, Neural Learning, Integrated Approach. Planning and Robotics: STRIPS, ABSTRIPS, NOAH and MOLGEN planners, preliminary ideas of distributed and real time planning, Subsumption architecture based planning. Expert Systems: fundamental blocks, case studies in various domains, concept of shells, connectionist expert systems. Introduction to Natural Language Understanding: problems of ambiguity, ellipsis and polysemy, lexicalization and parsing, Transition and Augumented Transition networks, Natural Language Interfaces. Introdution to Computer Vision: Edge detection, Point Correspondence and Stereopsis, Surface directions. Basics of Neural Networks: Perceptrons, Feedforward nets Backpropagation algorithm, preliminary understanding of unsupervised learning.

**Pre-requisites**
None

#### **CS726** : <u>Advanced machine learning</u>
**Syllabus** 
This course will concentrate on modeling, generation, and prediction of multiple inter-dependent variables. The topics covered will span probabilistics graphical models (directed and undirected), inference methods like junction trees, belief propagation, and other approximate methods, sampling methods like MCMC, variational auto-encoders, GANs, neural architectures for sequence and graph-structured predictions. When appropriate the techniques will be linked to applications in translation, conversation modeling, speed recognition, graphics, and science.

**Pre-requisites**
A formal introductory ML course like CS 725 or CS 337 or CS 419 is required. Online ML courses do not qualify as pre-requisites. The course assumes basic knowledge of probability, statistics, and linear algebra. Chapters 2 and 3 of the [Deep-learning book](http://www.deeplearningbook.org/) are a good place to refresh the necessary required background. Also, the course assumes basic background in machine learning, for example as covered in Chapter 5 of the [Deep-learning book](http://www.deeplearningbook.org/) and deep learning, for example, as covered in Chapter 6 of the same book. Further, we will assume that students are familiar with CNNs, RNNs, and sequence to sequence learning with attention.

#### CS 748 : <u> Advances in Intelligent and Learning Agents</u>

**Syllabus**
Artificial intelligence is fast making inroads into various fields, and is indeed influencing our lives in a telling way. This course will accustom students to the state-of-the-art in designing and deploying intelligent and learning agents. The course will build upon the platform laid by CS 747 (Foundations of Intelligent and Learning Agents) to engage students in targeted research and system-building projects.

The course has three main objectives. First, it seeks to impart students the mindset that artificial intelligence and machine learning can substantially benefit real-world problems, and give them the confidence that they can drive this process. Second, the course seeks to develop the students' skills to abstract, analyse, design, implement, evaluate, and iterate while devising solutions. The third objective of the course is to train students to comprehend technical discourse and sharpen their communication skills.

The course is organised in the form of two parallel tracks:  **class discussions**  based on research papers, and a semester-long  **research project**. Students will be provided a reading assignment every week, and will be expected to turn in a response summarising their understanding and related observations. Individual responses will be shared with the class, and will guide the class discussion, which will be led by a group of 2-3 designated students. Topics covered in the reading assignments will include, among others, (1) philosophy of AI, (2) animal behaviour, (3) POMDPs, (4) evolutionary computation, (5) representation discovery, (6) crowdsourcing, (7) contextual bandits, (8) theoretical analysis of MDP planning and learning, (9) Monte Carlo tree search, (10) game-playing, and (11) robotics.

The research project presents an opportunity to students for applying their learning in creative and imaginative ways to understand, build, and analyse systems. Both theoretical and empirical investigations may be undertaken. Students may work alone or in teams. Each team will be guided individually through the phases of the research project, but will share its progress with the class at designated intervals.

**Pre-requisites**
CS 747 and consent of instructor

#### CS 753 : <u>Automatic Speech Recognition</u>
**Syllabus**
Introduction to the statistical approach for automatic speech recognition (ASR)  
* Weighted Finite State Transducers and their Application to ASR  
* Acoustic Signal Processing for ASR  
* Acoustic models: Hidden Markov Models, Gaussian Mixture Models, Baum-Welch Maximum Likelihood  
Estimation  
* Discriminative Training of Acoustic Models: Maximum Mutual Information, Minimum Word/Phone Error Criteria  
* Acoustic models continued: Neural network models (Deep feed-forward neural networks, convolutional neural networks and recurrent neural networks)  
* Pronunciation models: Pronunciation dictionaries, grapheme-to-phoneme models, feature-based models  
* N-gram language models: estimation, smoothing  
* ASR decoding problem: search algorithms, Viterbi estimation, finite-state transducer optimizations  
Programming assignments for a few of the above listed topics, along with a final research project, will be part of the curriculum.

**Pre-requisites**
This course is open to 3rd and 4th year B.Tech., M.Tech. and Ph.D. students, who have passed a formal course in ML (offered by either the CSE, EE or IEOR department).

#### CS 772 : Deep Learning for Natural Language Processing
**Syllabus** 
Background: History of Neural Nets; History of NLP; Basic Mathematical Machinery- Linear Algebra, Probability, Information Theory etc.; Basic Linguistic Machinery- Phonology, morphology, syntax, semantics Introducing Neural Computation: Perceptrons, Feedforward Neural Network and Backpropagation, Recurrent Neural Nets Difference between Classical Machine Learning and Deep Learning: Representation- Symbolic Representation, Distributed Representation, Compositionality; Parametric and non-parametric learning Word Embeddings: Word2Vec (CBOW and Skip Gram), Glove, FastText Application of Word Embedding to Shallow Parsing- Morphological Processing, Part of Speech Tagging and Chunking Sequence to Sequence (seq2seq) Transformation using Deep Learning: LSTMs and Variants, Attention, Transformers Deep Neural Net based Language Modeling: XLM, BERT, GPT2-3 etc; Subword Modeling; Transfer Learning and Multilingual Modeling Application of seq2seq in Machine Translation: supervised, semi supervised and unsupervised MT; encoder-decoder and attention in MT; Memory Networks in MT Deep Learning and Deep Parsing: Recursive Neural Nets; Neural Constituency Parsing; Neural Dependency Parsing Deep Learning and Deep Semantics: Word Embeddings and Word Sense Disambiguation; Semantic Role Labeling with Neural Nets Indispensability of DNN in Multimodal NLP; Advanced Problems like Sarcasm, Metaphor, Humour and Fake News Detection using multimodality and DNN Natural Language Generation; Extractive and Abstractive Summarizationwith Neural Nets- Explainability

**Pre-requisites**
N/A

#### CS 769 :  <u>Optimization in Machine Learning</u>
**Syllabus** 
N/A

**Pre-requisites**
N/A

### EE Department

#### EE679 : <u>SPEECH PROCESSING</u>
**Syllabus**
Speech production and acoustic phonetics, speech perception.;Speech analysis: time and frequency domain techniques for pitch and formant estimation, cepstral and LPC analysis.;Speech synthesis: articulatory, formant, and LPC synthesis, voice response and text-to-speech systems.;Applications: data compression, vocoders, speech enhancement, speech recognition, speaker recognition, aids for the speech and hearing impairments.

**Pre-requisites**
Courses: Digital Signal Processing (EE 338 – 6th sem), Probability (EE325 – 5th sem)
Skills: MATLAB

#### EE704 : <u>ARTIFICIAL NEURAL NETWORKS</u>
**Syllabus**
Introduction: Biological neurons and memory: Structure and function of a single neuron; Artificial Neural Networks (ANN); Typical applications of ANNs : Classification, Clustering, Vector Quantization, Pattern Recognition, Function Approximation, Forecasting, Control, Optimization; Basic Approach of the working of ANN - Training, Learning and Generalization.;Supervised Learning: Single-layer networks; Perceptron-Linear separability, Training algorithm, Limitations; Multi-layer networks-Architecture, Back Propagation Algorithm (BTA) and other training algorithms, Applications. Adaptive Multi-layer networks-Architecture, training algorithms; Recurrent Networks; Feed-forward networks; Radial-Basis-Function (RBF) networks.;Unsupervised Learning: Winner-takes-all networks; Hamming networks; Maxnet; Simple competitive learning; Vector-Quantization; Counter propagation networks; Adaptive Resonance Theory; Kohonen's Self-organizing Maps; Principal Component Analysis.;Associated Models: Hopfield Networks, Brain-in-a-Box network; Boltzmann machine.;Optimization Methods: Hopfield Networks for-TSP, Solution of simultaneous linear equations; Iterated Gradient Descent; Simulated Annealing; Genetic Algorithm.

**Pre-requisites**
N/A

#### EE763 : <u>SCIENCE OF INFORMATION STATISTICS AND LEARNING</u>
**Syllabus**
Information Theory basics: Bayes’ theorem, Random Variables, Independence and Conditioning, Shannon entropy, Relative entropy, Mutual Information, Markov chains, Sanov’s theorem.
Statistics: Linear regression, statistical model, Exponential families, sampling, Monte Carlo, inference, Maximum Likelihood Estimation, Maximum a posteriori, Bayesian Inference.
Inference: MaxENT algorithm, relation between Bayesian and MaxENT methods, Statistical Mechanics, Ising models, graphical models, Hammersley-Clifford theorem, EM algorithm, belief propagation.
Learning: Introduction to neural networks, the single neuron as a classifier, capacity of a single neuron, learning as inference, Hopfield networks, Boltzmann machines, Supervised learning in multilayered networks, Gaussian processes, Deconvolution.
Application to Chemical Reaction Networks: Introduction to chemical reaction networks, Mass-action kinetics, Chemical Master Equation, Birch’s theorem, Connection to exponential families, the MLE algorithm using reaction networks, current topics in molecular intelligence.

**Pre-requisites**
  a) Python programming/ PyTorch 
 b) Basics of Machine Learning including tensors, autograd, optimization by gradient descent, loss functions, neural network basics, training and validation sets.
 c) Some basic introduction to statistics including maximum likelihood estimation and Bayesian inference.
 d) Probability theory at a naive level, Markov chains, and basic information theory at the level of Chapter 2 of "Elements of Information Theory" by Cover and Thomas.

#### EE771:  <u>RECENT TOPICS IN ANALYTICAL SIGNAL PROCESSING</u>
**Syllabus**
Compressed sensing, finite rate of innovation signals and their sampling methods, graph signal processing and its applications, phase retrieval problems, distributed sampling problems, machine learning for signal processing, role of quantization and other nonlinearities in signal processing systems, signal approximation methods

**Pre-requisites**
A basic DSP course would be helpful to understand the concepts taught in the course – EE338 (although not a hard pre-requisite)

#### EE782 : <u>ADVANCED MACHINE LEARNING</u>
Penalized and generalized linear models: Logistic regression, L1 and L2 penalization, elastic net, SCAD penalty, application to high dimensional low sample size problems. Intro to neural networks: Artificial neuron, single hidden layer, multiple hidden layer, back propagation, momentum, loss functions, relation with support vector machines and penalized logistic regression. Convolutional neural networks: Convolutional layers, pooling layers, drop out, VGGnet, inception modules, residual networks, deconv nets, applications to object recognition. Why deep learning works: Role of depth, closeness of local minima to global minimal, predominance of saddle points and ridges vs. local minima. Recurrent neural networks and LSTMs: lateral connections, LSTM units, gated recurrent networks, applications to NLP. Probabilistic Graphical Models: Factor graphs and belief networks, Deep belief networks and Boltzmann machines, sampling methods including Gibbs sampling, contrastive divergence, generative adversarial networks.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIwMTk0NDczMjQsMTgwMDMyODgxNl19
-->